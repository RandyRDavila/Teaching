{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The K-Nearest Neighbors Algorithm\n",
    "\n",
    "The **$k$-nearest neighbors algorithm** (or KNN for short) is a simple supervised machine learning algorithm that assumes that similar things exist in close proximity. In other words, similar things are near to each other.\n",
    "For example, conisder the [iris data set](https://en.wikipedia.org/wiki/Iris_flower_data_set) in by the code in the first code cell below. \n",
    "\n",
    "For this notebook we will need the following packages:\n",
    " * RDatasets [documentation](https://github.com/JuliaStats/RDatasets.jl)\n",
    " * Plots [documentation](http://docs.juliaplots.org/latest/)\n",
    " * CSV [documentation](https://juliadata.github.io/CSV.jl/stable/)\n",
    " \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using RDatasets\n",
    "using Plots\n",
    "using CSV\n",
    "\n",
    "iris = dataset(\"datasets\", \"iris\")\n",
    "X_data = [x for x in zip(iris.SepalLength, iris.SepalWidth, iris.PetalLength)]\n",
    "Y_data = [iris.Species[i] for i = 1:150]\n",
    "\n",
    "\n",
    "scatter(xaxis = \"Sepal Length\",\n",
    "            yaxis = \"Sepal Width\", \n",
    "            zaxis = \"Petal Length\",\n",
    "            title = \"Iris Data Set 3D Scatter Plot\")\n",
    "\n",
    "    scatter!(X_data[1:50], \n",
    "            label = \"setosa\", \n",
    "            color = \"lightblue\")\n",
    "    scatter!(X_data[51:100], \n",
    "            label = \"versicolor\", \n",
    "            color = \"lightyellow\")\n",
    "    scatter!(X_data[101:150], \n",
    "            label = \"virginica\", \n",
    "            color = \"lightgreen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In the plot generated by the code cell above (code cell [1]), data points tend to be close to other data points of a similar class (or label). This notion is what the KNN algorithm needs in order to be useful at predicting a given label. Specifically, the KNN algorithm captures the idea of similarity  by calculating the distance between points on a graph.\n",
    "\n",
    "There are many ways of calculating distance (in the mathematics community these are called metrics), and one way might be preferable to another way depending on the specific problem that we are solving. However, the straight-line distance (also called the Euclidean distance) is a popular and simple choice.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Euclidean distance formula as a function\n",
    "function euclidean_distance(p1, p2)\n",
    "    return sqrt(sum([(p1[i] - p2[i])^2 for i = 1:length(p1)]))\n",
    "end\n",
    "\n",
    "# Test the function euclidean_distance to make sure it works!\n",
    "print(\"The distance between \", X_data[1],\" and \", X_data[50])\n",
    "println(\" is \", euclidean_distance(X_data[1], X_data[50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "**The KNN Algorithm**\n",
    "\n",
    "1. Load the data\n",
    "\n",
    "2. Initialize $k$ to your chosen number of neighbors\n",
    "\n",
    "3. For each example in the data\n",
    "\n",
    " * Calculate the distance between the query example and the current example from the data.\n",
    "\n",
    " * Add the distance and the index of the example to an ordered collection\n",
    "\n",
    "4. Sort the ordered collection of distances and indices from smallest to largest (in ascending order) by the distances\n",
    "\n",
    "5. Pick the first $k$ entries from the sorted collection\n",
    "\n",
    "6. Get the labels of the selected $k$ entries\n",
    "\n",
    "7. If regression, return the mean of the $k$ labels\n",
    "\n",
    "8. If classification, return the mode of the $k$ labels\n",
    "\n",
    "\n",
    "**Advantages**\n",
    "\n",
    "* The algorithm is simple and easy to implement.\n",
    "* There’s no need to build a model, tune several parameters, or make additional assumptions.\n",
    "* The algorithm is versatile. It can be used for classification and regression.\n",
    "\n",
    "**Disadvantages**\n",
    "\n",
    "* The algorithm gets significantly slower as the number of examples and/or predictors/independent variables increase.\n",
    "\n",
    "In the next code cell we write a function that computes the $k$-nearest neighbors to a given point $p$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" For a given point p, a collection of points X with labels Y, and a positive integer k,\n",
    "    find the k nearest neighbors to point p.\n",
    "\"\"\"\n",
    "function k_nearest_neighbors(p, X, Y, k)\n",
    "    # Calculate the distance between p and all other points in X\n",
    "    distance_array = [(X[i], Y[i], euclidean_distance(p, X[i])) \n",
    "                        for i = 1:length(X)\n",
    "                        if X[i] != p]\n",
    "    # Sort the distance array in ascending order according to distance\n",
    "    sort!(distance_array, by = x->x[3])\n",
    "    \n",
    "    # Return the first k entries from the sorted distance array \n",
    "    return distance_array[1:k]\n",
    "end \n",
    "\n",
    "# Test the k_nearest_neighbors function\n",
    "println(\"Target Point P = \", X_data[120])\n",
    "println(\"k = \", 5)\n",
    "println(\"________________________\")\n",
    "for x in k_nearest_neighbors(X_data[120], X_data, Y_data, 5)\n",
    "    println(\"Point = \", x[1])\n",
    "    println(\"Point Label = \", x[2])\n",
    "    println(\"Point Distance = \", x[3])\n",
    "    println(\"\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "**Choosing the right value for $k$**\n",
    "\n",
    "To select the $k$ that’s right for your data, we run the KNN algorithm several times with different values of $k$. Choose the $k$ that reduces the number of errors we encounter while maintaining the algorithm’s ability to accurately make predictions when it’s given data it hasn’t seen before.\n",
    "\n",
    "Here are some things to keep in mind:\n",
    "\n",
    "* As we decrease the value of $k$ to 1, our predictions become less stable. Just think for a minute, imagine $k = 1$ and we have a query point surrounded by several yellos and one green (see the iris plot above generated by code cell [1]), but the green is the single nearest neighbor. Reasonably, we would think the query point is most likely yellow, but because $k = 1$, KNN incorrectly predicts that the query point is green.\n",
    "    \n",
    "* Inversely, as we increase the value of $k$, our predictions become more stable due to majority voting / averaging, and thus, more likely to make more accurate predictions (up to a certain point). Eventually, we begin to witness an increasing number of errors. It is at this point we know we have pushed the value of $k$ too far. In cases where we are taking a majority vote (e.g. picking the mode in a classification problem) among labels, we usually make $k$ an odd number to have a tiebreaker.\n",
    "\n",
    "In the next code cell (code cell [4]), we write a function that both makes a prediction and also allows visualization of taking different values of $k$. \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function predict(index, X, Y, k; point_color = \"red\", show_img = false)\n",
    "    \n",
    "    point = X[index]\n",
    "    neighbors = k_nearest_neighbors(point, X, Y, k)\n",
    "    \n",
    "    if show_img == true\n",
    "        scatter(xaxis = \"Sepal Length\",\n",
    "                yaxis = \"Sepal Width\", \n",
    "                zaxis = \"Petal Length\",\n",
    "                title = \"Iris Data Set 3D Scatter Plot\")\n",
    "    end\n",
    "    \n",
    "    for x in neighbors\n",
    "        plot!([point, x[1]], color = point_color, label = false)\n",
    "    end\n",
    "    \n",
    "    if show_img == true\n",
    "        scatter!(X[1:50], \n",
    "                label = \"setosa\", \n",
    "                color = \"lightblue\")\n",
    "        scatter!(X[51:100], \n",
    "                    label = \"versicolor\", \n",
    "                color = \"lightyellow\")\n",
    "        scatter!(X[101:150], \n",
    "                label = \"virginica\", \n",
    "                color = \"lightgreen\")\n",
    "        scatter!(point, color = point_color, label = \"target\")\n",
    "    end\n",
    "\n",
    "    setosa_count = sum([1.0 for x in neighbors if x[2] == \"setosa\"])\n",
    "    versicolor_count = sum([1.0 for x in neighbors if x[2] == \"versicolor\"])\n",
    "    virginica_count = sum([1.0 for x in neighbors if x[2] == \"virginica\"])\n",
    "    println(\"     P(setosa)       |     P(versicolor)       |     P(virginica)\")\n",
    "    println(\"----------------------------------------------------------------\")\n",
    "    print(\"        \", setosa_count/k,\"          |          \", versicolor_count/k)\n",
    "    print(\"            |        \", virginica_count/k)\n",
    "    println(\" \")\n",
    "    println()\n",
    "    \n",
    "    \n",
    "    if show_img == true\n",
    "            scatter!()\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the predict function to make sure it works!\n",
    "predict(90, X_data, Y_data, 7, show_img = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "**Recommendation Example**\n",
    "\n",
    "Imagine for a moment. We are navigating the MoviesXb website, a fictional IMDb spin-off, and we encounter The Avengers. We aren’t sure we want to watch it, but its genres intrigue us; we are curious about other similar movies. We scroll down to the “More Like This” section to see what recommendations MoviesXb will make, and the algorithmic gears begin to turn... \n",
    "\n",
    "Write a function that provides us with a recommended list of \"More Like This\" movies using the KNN algorithm.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First read the csv file from your directory\n",
    "movies = CSV.read(\"movies_recommendation_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = [x for x in zip(movies.IMDBRating, movies.Biography, movies.Drama, movies.Thriller)]\n",
    "Y_data = [x for x in movies.MovieName];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function more_like_this(movie_name, X, Y, k)\n",
    "    \n",
    "    for i = 1:length(Y)\n",
    "        if Y[i] == movie_name\n",
    "            L = k_nearest_neighbors(X[i], X, Y, k)\n",
    "            println(\"The top $k similar movies with $movie_name are:\")\n",
    "            for j = 1:k\n",
    "                println(\"$j. \", L[j][2])\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_like_this(\"A Beautiful Mind\", X_data, Y_data, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Summary**\n",
    "\n",
    "The $k$-nearest neighbors (KNN) algorithm is a simple, supervised machine learning algorithm that can be used to solve both classification and regression problems. It’s easy to implement and understand, but has a major drawback of becoming significantly slows as the size of that data in use grows.\n",
    "\n",
    "KNN works by finding the distances between a query and all the examples in the data, selecting the specified number examples ($k$) closest to the query, then votes for the most frequent label (in the case of classification) or averages the labels (in the case of regression).\n",
    "\n",
    "In the case of classification and regression, we saw that choosing the right $k$ for our data is done by trying several Ks and picking the one that works best."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.5",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
